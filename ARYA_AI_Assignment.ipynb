{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ARYA.AI_Assignment.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPsfI2+HKI/FH7kZmQy6xNR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vardhan1379/ARYA.ai-assignment/blob/main/ARYA_AI_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis**\n",
        "\n",
        "---\n",
        "**This dataset contains 57 features named from X1 to x57 and a label column named Y. While observing the discreteness in the label's column, we can consider its an classification problem.While observing the data I can conclude that there are lot of zeros in the data,which isn't a proper distribution to frame a model i.e.,This model might have an tendency to zero inflation. To avoid this, standardization of data is done using standard scaler. In general, feature selection is crucial step in data engineering although there are no specified feature names in this data we perform chi-squared test to determine the 35 most important features while determining Y( feature selection is used to avoid the curse of dimensionality and to maintain Occam's razor).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oknFbzqy_6RW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Required Libraries**"
      ],
      "metadata": {
        "id": "sLDmm4wCAjZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dVR1Ru1_5pS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from numpy import asarray\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier,XGBRFClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read Data**"
      ],
      "metadata": {
        "id": "LG2bP5sCD2Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data = pd.read_csv(\"training_set.csv\",delimiter=\",\",index_col=0)\n",
        "print(Data)\n",
        "Test_data = pd.read_csv(\"test_set.csv\",delimiter=\",\",index_col=0)\n",
        "print(Test_data)"
      ],
      "metadata": {
        "id": "Q8se80yTD-89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get information of the Data** "
      ],
      "metadata": {
        "id": "f-1qLjR_HpR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data.describe()"
      ],
      "metadata": {
        "id": "4_XeMorCIGps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**while watching the above results we can conclude that\" There are outliers in our data\" because there is massive difference b/w 75 th percentile and max values.**\n",
        "\n",
        "---\n",
        "\n",
        "Note: I have tried to include various plots such as t-sne, PCA plots and Box plots. Due to high number of features there is problem of overlapping of features in the plots. So I've decided to remove them and tried to understand the Data based on Corelation Heat map and Data dispersion quatifiers."
      ],
      "metadata": {
        "id": "GbWYc3XIJ10Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data.info()"
      ],
      "metadata": {
        "id": "0aR9TZNzIOwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can see that there are no NULL values in any of the attributes and it only consists of integers and float values.**\n",
        "\n",
        "---\n",
        "**Now we will plot the correlation heat map for the data**\n"
      ],
      "metadata": {
        "id": "fwUKITWLKVUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(100,100))\n",
        "sns.heatmap(Data.iloc[:,:-1].corr(),cmap='viridis',annot=True) "
      ],
      "metadata": {
        "id": "Ypvu3OlYOnRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can observe there are few highly positive correlated and Few Highly Negative correlated variables, Using Chi-squared test we can remove Multicollinearity**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HCu3ZX5gSvpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets Check for imbalance data**"
      ],
      "metadata": {
        "id": "72hZJmjRCtbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_count = Data.Y.value_counts()\n",
        "print('Class 0:', target_count[0])\n",
        "print('Class 1:', target_count[1])\n",
        "print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n",
        "\n",
        "target_count.plot(kind='bar', title='Count (target)');"
      ],
      "metadata": {
        "id": "2Xj49AmZEX5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Preprocess the Data**"
      ],
      "metadata": {
        "id": "2XTNebS0HVl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we use data for feature selection step. This feature selcton step avoids multi-collinearity between the features while improving the performance of model **"
      ],
      "metadata": {
        "id": "PhuTdF11LlQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "select = SelectKBest(score_func=chi2, k=35)\n",
        "z = select.fit_transform(Data.iloc[:,:-1],Data.iloc[:,-1])\n",
        "feature_names =  select.get_feature_names_out(Data.iloc[:,:-1].columns)\n",
        "print(\"Important faeture names are\",feature_names)\n",
        "print(\"Shape of Train data after Feature selection:\", z.shape)\n",
        "Data_sel = Data[feature_names]\n",
        "print(Data_sel.shape)\n",
        "#Now remove the less important features in test data too\n",
        "Test_data_sel = Test_data[feature_names]\n",
        "print(\"Shape of test data after feature selection is\",Test_data_sel.shape)"
      ],
      "metadata": {
        "id": "h0WCVcdOLx1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standardization using standard scaler**"
      ],
      "metadata": {
        "id": "NJN5CWrUKXCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "#Transform data\n",
        "Data_scaled = scaler.fit_transform(Data_sel)\n",
        "print(\"Scaled Data shape:\",Data_scaled.shape)\n",
        "print(Data_sel)\n",
        "#sacling the test data\n",
        "Test_data_scaled = scaler.fit_transform(Test_data_sel)\n",
        "print(\"Scaled test data shape:\",Test_data_scaled.shape)\n",
        "print(Test_data_scaled)"
      ],
      "metadata": {
        "id": "D5Duj5-kHTeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now split the \"Data\" into training set and validation set**"
      ],
      "metadata": {
        "id": "aVjOAezdGS0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we ready the whole Data into Train, test and validation data\n",
        "X_train, X_val, y_train, y_val = train_test_split(Data_scaled, Data.iloc[:,-1], test_size=0.2, random_state=1)\n",
        "Testing_data = Test_data_scaled\n",
        "print(\"Training_data shape:\",X_train.shape,y_train.shape)\n",
        "print(\" Validation_data shape:\",X_val.shape,y_val.shape)\n",
        "print(\"Test _data shape:\",Testing_data.shape)\n"
      ],
      "metadata": {
        "id": "_V589gjJW_EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Validation**\n",
        "\n",
        "---\n",
        "**1. Models that are used to check the individual performance**\n"
      ],
      "metadata": {
        "id": "Sdzg3z2wdz6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "models.append(('KNN-1', KNeighborsClassifier(n_neighbors=1)))\n",
        "models.append(('KNN-3', KNeighborsClassifier(n_neighbors=3)))\n",
        "models.append(('KNN-5', KNeighborsClassifier(n_neighbors=5)))\n",
        "models.append(('KNN-7', KNeighborsClassifier(n_neighbors=7)))\n",
        "models.append(('KNN-9', KNeighborsClassifier(n_neighbors=9)))\n",
        "models.append(('KNN-11', KNeighborsClassifier(n_neighbors=11)))\n",
        "models.append(('KNN-13', KNeighborsClassifier(n_neighbors=13)))\n",
        "models.append(('AB', AdaBoostClassifier()))\n",
        "models.append(('GBM', GradientBoostingClassifier()))\n",
        "models.append(('RF', RandomForestClassifier()))\n",
        "models.append(('ET', ExtraTreesClassifier()))\n",
        "models.append((\"BC\",BaggingClassifier()))\n",
        "models.append((\"XGBC\",XGBClassifier()))\n",
        "\n"
      ],
      "metadata": {
        "id": "Gw5iwrzrbiQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Performance analysis metrics**"
      ],
      "metadata": {
        "id": "mgnbBYMDe-Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotconfusion_matrix(modelname,y_test,y_pred):\n",
        "   \n",
        "    conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "    print(modelname+'Confusion matrix:\\n', conf_mat)\n",
        "    labels = ['Class 0', 'Class 1']\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticklabels([''] + labels)\n",
        "    ax.set_yticklabels([''] + labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Expected')\n",
        "    plt.show()\n",
        "model_lis= []\n",
        "def modelfitpredict(modelname,model,X_train, X_test, y_train, y_test):\n",
        "    \n",
        "    \n",
        "    model = model\n",
        "    model.fit(X_train,y_train)\n",
        "    model_lis.append(model)\n",
        "    y_pred = model.predict(X_test)\n",
        "    plotconfusion_matrix(name,y_test,y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(name+\" Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    print(name+' ROC AUC: %f' % auc)\n",
        "    print(\"-\"*50)\n",
        "    #results.update( {name : accuracy*100.0} )\n",
        "    return name, accuracy, model_lis"
      ],
      "metadata": {
        "id": "FqBFgvxGaHxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Training and performance evaluation of each models**"
      ],
      "metadata": {
        "id": "Fv-zWOkLfR7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for name, model in models:\n",
        "    name_, accuracy, model_s = modelfitpredict(name,model,X_train, X_val, y_train, y_val)\n",
        "    results[name_] = accuracy*100.0\n",
        "    \n",
        "print(\"Validation results on test data :\",results)"
      ],
      "metadata": {
        "id": "6ZdooubSfQQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Accuarcy comparision of various models**"
      ],
      "metadata": {
        "id": "-RwuY7EVrp79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)\n",
        "plt.bar(*zip(*results.items()))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P4HwwRQBhqaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The highest scoring algorithms are RandomForest and ETree classifiers which they have attained an acuarcy of 95.26%. Also we can see the accuracies of various classifiers is merely ditinguishable, But the performance of Decision Trees, boosting and bagging classifiers are the highest (They have the advantage of making multiple decisions and producing their decision as collective decision makes them high scorers in this sceneraio). Although, having advantageous classifiers in competition, Logistic Regression classifier performed exceptionally well for such an simple algorithm.(It performed well because of the step \"Feature Selection\". This Feature Selection helped the model to retain unnecessary variables in equation which in then helped the logistic regression model to achieve the accuarcy of 91.94%).**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Now we will use the RandomForest classifier to predict the Test Set**"
      ],
      "metadata": {
        "id": "dYUsX7NYn1GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(Testing_data)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "DRyO1i3VrAn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now above values are the predicted values for test dataset**"
      ],
      "metadata": {
        "id": "PyGRe8W2x3n4"
      }
    }
  ]
}